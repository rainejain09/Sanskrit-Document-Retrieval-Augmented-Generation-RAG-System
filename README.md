
# ğŸ“œ Sanskrit Document Retrieval-Augmented Generation (RAG) System

# ğŸ“Œ Project Overview
This project implements a Retrieval-Augmented Generation (RAG) system capable of answering user queries based on Sanskrit documents.
The system integrates semantic retrieval with a CPU-based Large Language Model (LLM) to generate context-grounded responses, ensuring factual correctness and reduced hallucination.
The entire pipeline is designed to run without GPU support, making it lightweight, reproducible, and compliant with CPU-only constraints.

# ğŸ¯ Objectives
Ingest Sanskrit documents in Unicode format

Preprocess and index documents for efficient retrieval

Accept queries in Sanskrit (Devanagari)

Retrieve relevant context from the corpus

Generate accurate, grounded answers using a CPU-based LLM

# ğŸ§  System Architecture
User Query (Sanskrit)
        â†“
Query Embedding
        â†“
FAISS Vector Retrieval
        â†“
Relevant Sanskrit Chunks
        â†“
Prompt Construction
        â†“
CPU-based LLM (mT5)
        â†“
Generated Answer

# ğŸ› ï¸ Tech Stack
Component	Technology
Programming Language	Python 3
Document Parsing	python-docx
Text Chunking	LangChain Text Splitters
Embeddings	Multilingual MiniLM
Vector Store	FAISS (CPU)
Language Model	mT5-small
Deployment	Google Colab / Local CPU
ğŸ“‚ Project Structure
RAG_Sanskrit_<YourName>/
â”‚
â”œâ”€â”€ code/
â”‚   â””â”€â”€ Sanskrit_RAG_System.ipynb
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Rag-docs.docx
â”‚
â”œâ”€â”€ report/
â”‚   â””â”€â”€ Sanskrit_RAG_Report.pdf
â”‚
â””â”€â”€ README.md

# ğŸš€ Setup Instructions
ğŸ”¹ 1. Clone the Repository
git clone https://github.com/<your-username>/RAG_Sanskrit_<YourName>.git
cd RAG_Sanskrit_<YourName>

ğŸ”¹ 2. Environment Requirements

Python 3.9+

CPU-only environment (no GPU required)

ğŸ”¹ 3. Install Dependencies

If running locally:

pip install torch transformers sentence-transformers faiss-cpu \
langchain langchain-community langchain-text-splitters python-docx


For Google Colab:

Use CPU runtime

Install dependencies inside the notebook

# â–¶ï¸ How to Run

Open the notebook:

code/Sanskrit_RAG_System.ipynb

Run cells sequentially:

Library installation

Document upload

Preprocessing & chunking

FAISS indexing

Query execution

Enter a Sanskrit query, for example:

à¤¶à¤‚à¤–à¤¨à¤¾à¤¦à¤ƒ à¤•à¤¿à¤®à¥ à¤•à¥ƒà¤¤à¤µà¤¾à¤¨à¥ ?


The system retrieves relevant context and generates an answer grounded in the document.

# âš™ï¸ CPU Optimization Notes

No GPU or CUDA usage

Lightweight multilingual embedding model

Small LLM (mT5-small)

FAISS-CPU vector indexing

Minimal memory footprint

# ğŸ“Š Performance Observations

Retrieval latency: ~40â€“70 ms

Generation latency: ~1.5â€“2.5 seconds

RAM usage: ~2â€“3 GB

High answer relevance for document-based queries

# âš ï¸ Limitations

No sandhi splitting

Limited Sanskrit-specific LLM availability

Transliteration normalization not included

# ğŸ”® Future Enhancements

Hybrid BM25 + vector retrieval

Sandhi and morphological analysis

Transliteration (IAST â†” Devanagari)

Web UI using Streamlit

Sanskrit-fine-tuned LLM

# ğŸ“„ Report

A detailed technical report describing system architecture, preprocessing, retrieval, generation, and performance analysis is available in:

/report/Sanskrit_RAG_Report.pdf
